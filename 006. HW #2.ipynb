{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages importing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing variable info func\n",
    "def getInfo(val):\n",
    "    print(type(val))\n",
    "    try:\n",
    "        print(val)\n",
    "        if str(type(val)) == \"<class 'numpy.ndarray'>\" or \\\n",
    "           str(type(val)) == \"<class 'pandas.core.frame.DataFrame'>\":\n",
    "            print(val.shape)\n",
    "        else:\n",
    "            print(len(val))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Plotting func\n",
    "def plot_acc(h, title='accuracy'):\n",
    "    plt.plot(h.history['accuracy'])\n",
    "    plt.plot(h.history['val_accuracy'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training', 'Validation'], loc=0)\n",
    "\n",
    "def plot_loss(h, title='loss'):\n",
    "    plt.plot(h.history['loss'])\n",
    "    plt.plot(h.history['val_loss'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training', 'Validation'], loc=0)\n",
    "\n",
    "def plot_loss_acc(h, title='accuracy & loss'):\n",
    "    plt.plot(h.history['accuracy'])\n",
    "    plt.plot(h.history['val_accuracy'])\n",
    "    plt.plot(h.history['loss'])\n",
    "    plt.plot(h.history['val_loss'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuract & Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training_acc', 'Validation_acc', 'Training_loss', 'Validation_loss'], loc=0)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preparation for wdbc.data (Not Used)\n",
    "\n",
    "# data = pd.read_csv(\"./hw2_data/wdbc.data\", header=None)\n",
    "# # getInfo(data)\n",
    "\n",
    "# # Data Preparation\n",
    "# data = data.drop(data.columns[0], axis=1)                               # remove ID col\n",
    "# data = data.astype(str)\n",
    "# data[data.columns[0]] = data[data.columns[0]].map({'M': 1, 'B': 0})     # 2 for benign(0), 4 for malignant(1)\n",
    "# # getInfo(data)\n",
    "\n",
    "# x = data[data.columns[2:12]].values\n",
    "# getInfo(x)\n",
    "# y = data[data.columns[0]].values\n",
    "# getInfo(y)\n",
    "\n",
    "# # Data normalization\n",
    "# scaler = MinMaxScaler()\n",
    "# x = scaler.fit_transform(x)\n",
    "# getInfo(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, pandas library 의 read_csv()를 통해 제공된 데이터를 읽어왔습니다.\n",
    "그런데 breast-cancer-wisconsin.data 의 경우 첫번째 line부터 바로 raw data가 있고, 이 값들이 DataFrame의 col이 되어 첫 줄의 Data가 누락되는 현상을 방지하기 위해 header의 parameter 값을 None으로 설정해 주었습니다.\n",
    "\n",
    "이 데이터에 대한 설명을 제공하는 breast-cancer-wisconsin.names라는 파일의 \"7. Attribute information\" 부분을 주목하면,\n",
    "\n",
    "1번째 col은 ID, 2번째부터 10번째 col은 각각의 데이터가 보관되어 있으며\n",
    "그리고 마지막 11번째 col은 양성인지 악성인지 판별하는 class (output) 이 보관되어 있다고 명시되어 있습니다.\n",
    "\n",
    "때문에 이번 학습에 사용되지 않는 첫번째 col 의 값을 drop() 를 이용해 제거했으며, 2번째부터 10번째까지의 data들을 input(x), 11번째(마지막)의 data들을 output(y) 라고 하였습니다.\n",
    "\n",
    "그런데 마지막 col 에서 benign는 2, malignant는 4라고 저장되어 있기에 DataFrame type의 array를 map()을 이용해 0 또는 1로 새롭게 mapping 해주었습니다.\n",
    "\n",
    "\n",
    "다음으로 missing value 처리를 위해 \"?\"가 포함된 row를 replace()를 통해 np.nan으로 변경하고 dropna() 수행했습니다.\n",
    "결과적으로 16개의 sample을 제거해서 699개에서 683개의 샘플로 줄은 것을 확인할 수 있습니다.\n",
    "\n",
    "그 이후 MinMaxScaler를 이용해 normalizing 과정을 거쳤으며 반환된 numpy.ndarray의 변수를 제시한 조건에 맞게 split 시켜서 test, validation, train 을 위한 데이터를 구분했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preparation\n",
    "\n",
    "# Get RAW Data\n",
    "data = pd.read_csv(\"./hw2_data/breast-cancer-wisconsin.data\", header=None)\n",
    "# getInfo(data)\n",
    "\n",
    "data = data.drop(data.columns[0], axis=1)                               # remove ID col\n",
    "data = data.astype(str)\n",
    "data = data.replace(['?'], np.nan).dropna()                             # Missing Value Processing\n",
    "data[data.columns[-1]] = data[data.columns[-1]].map({'4': 1, '2': 0})   # 2 for benign(0), 4 for malignant(1)\n",
    "# getInfo(data)\n",
    "\n",
    "x = data[data.columns[:-1]].values\n",
    "# getInfo(x)\n",
    "y = data[data.columns[-1]].values\n",
    "# getInfo(y)\n",
    "\n",
    "# Data normalization\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "# getInfo(x)\n",
    "\n",
    "# Test Validation Data split\n",
    "x_test = x[0:100]\n",
    "y_test = y[0:100]\n",
    "x_val = x[100:200]\n",
    "y_val = y[100:200]\n",
    "x_train = x[200:]\n",
    "y_train = y[200:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN의 modeling과 train, evaluate를 모두 수행한 후 학습 결과를 저장한 ndarray type의 array를 반환하는 함수입니다.\n",
    "뒤의 과정(Q2~Q4) 에 여러 trial을 거쳐서 학습을 수행해야 했기 때문에, trials 를 parameter 로 받고 반복 횟수를 지정하여 학습할 수 있도록 하였으며,\n",
    "hl_activation과 hl_neurons 또한 parameter로 받아서 modeling 할 때 activation function과 hidden neuron의 개수도 지정할 수 있도록 하였습니다.\n",
    "\n",
    "이 함수는 package import 아래가 아닌 중간 부분에 선언되었는데,\n",
    "이미 input data는 x로 정해져 있고, 불필요한 Preparation 과정을 막고자 input data를 parameter에 넣지 않고 global variable로 처리하여 input data를 주었습니다.\n",
    "\n",
    "초기에 학습 결과를 저장할 result라는 각각의 label만 존재하는 빈 array를 선언하였고, for 문을 사용하여 argument로 받아온 trial만큼 반복을 수행하도록 하였습니다.\n",
    "\n",
    "callbacks의 옵션을 주어 early-stopping 할 수 있도록 overfitting을 방지했으며 실제로 early-stopping을 하지 않고 모든 epoch 200회를 채웠을 때보다 accuracy가 더 좋게 나온 것을 확인했습니다.\n",
    "\n",
    "문제에서 요구한 대로 2 epoch 동안 val_loss의 개선이 없으면 중단되도록 patience의 값을 2로 주었으며 불필요한 epoch 진행과정을 보여주는 것을 생략하기 위해 verbose 의 값을 0으로 주었습니다.\n",
    "\n",
    "train이 모두 끝난 후 결과를 리턴하기 위해서 epoch가 끝나기 직전 history에 저장된 마지막 번째의 loss 값과 accuracy의 값을 사용했으며,\n",
    "한 번의 trial 이 끝나고 만들어진 train set 과 test set의 metrics들을 출력시키고, res라는 ndarray 에 담아 초기에 선언한 result에 append 시켰습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(trials=1, hl_activation='relu', hl_neurons=10):\n",
    "    \n",
    "    # Empty Table for result\n",
    "    result=np.array([\"Training loss\", \"Training accuracy\", \"Test loss\", \"Test accuracy\"]).reshape(1, 4)\n",
    "    \n",
    "    for trial in range(trials):\n",
    "      # Neural Network Modeling\n",
    "      model = models.Sequential()\n",
    "      model.add(layers.Dense(hl_neurons, activation=hl_activation, input_shape=(9,))) # input layer & HL\n",
    "      model.add(layers.Dense(1, activation='sigmoid'))                                # output layer\n",
    "      model.compile(optimizer='rmsprop',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "      \n",
    "      # Training w/o early-stopping\n",
    "      # history = model.fit(x_train, y_train, \n",
    "      #                     epochs=200, batch_size=10, validation_data=(x_val, y_val), verbose=0)\n",
    "\n",
    "      # Training w/ early-stopping\n",
    "      history = model.fit(x_train, y_train, \n",
    "                              epochs=200, batch_size=10, validation_data=(x_val, y_val), \n",
    "                              callbacks = [EarlyStopping(monitor='val_loss', patience=2)], verbose=0)\n",
    "\n",
    "      # Evaluating with test data\n",
    "      test_loss, test_acc = model.evaluate(x_test,y_test)\n",
    "\n",
    "      # Visualization\n",
    "      print(\"Trial #\", trial+1)\n",
    "      print(\"Training loss=\", history.history['loss'][-1],\\\n",
    "            \"\\nTraining accuracy=\", history.history['accuracy'][-1],\\\n",
    "            \"\\nTest loss=\", test_loss,\\\n",
    "            \"\\nTest accuracy=\", test_acc)      \n",
    "      res = np.array([history.history['loss'][-1], history.history['accuracy'][-1], test_loss, test_acc]).reshape(1, 4)\n",
    "      result = np.append(result, res, axis=0)\n",
    "      \n",
    "      # Plotting\n",
    "      plot_loss_acc(history)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 column 에 대해 평균과 표준편차를 구하는 함수입니다.\n",
    "\n",
    "맨 첫 번째 row에는 label 이 담겨있으므로 이것을 제외한 나머지 row 를 선택하고 for 문에서 iter로 사용된 col로 원하는 column을 선택했으므로 ndarray[1:, col] 라고 하였으며\n",
    "이것을 float type로 바꾸어주어 평균과 표준편차를 구하기 위한 연산을 할 수 있게 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMean_SD(ndarray):\n",
    "    # iterate with # of col\n",
    "    for col in range(ndarray.shape[1]):\n",
    "        mean = np.mean(ndarray[1:, col].astype('float32'))\n",
    "        std = np.std(ndarray[1:, col].astype('float32'))\n",
    "        print(ndarray[0, col], \"\\nMean=\", mean, \"SD=\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "Q2 = train_and_evaluate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 (Affection of HL activation function)\n",
    "hl_activation = [None, 'relu', 'sigmoid', 'tanh']\n",
    "Q3 = [getMean_SD(train_and_evaluate(trials=10, hl_activation=activation, hl_neurons=10)) for activation in hl_activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 (Affection of HL Neurons #, roughly)\n",
    "hl_neurons = [2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000]\n",
    "Q4 = [getMean_SD(train_and_evaluate(trials=5, hl_activation='relu', hl_neurons=neurons)) for neurons in hl_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QE1 (Affection of HL Neurons #, fine tuned)\n",
    "hl_neurons = [25, 30, 35, 40, 45]\n",
    "QE1 = [getMean_SD(train_and_evaluate(trials=1, hl_activation='relu', hl_neurons=neurons)) for neurons in hl_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QE2\n",
    "def getWeight(trials=1): \n",
    "    # Neural Network Modeling\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(1, input_shape=(9,), activation='sigmoid')) # input & output layer\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    # Training w/ early-stopping\n",
    "    history = model.fit(x_train, y_train, \n",
    "                            epochs=200, batch_size=10, validation_data=(x_val, y_val), \n",
    "                            callbacks = [EarlyStopping(monitor='val_loss', patience=2)], verbose=0)\n",
    "\n",
    "    # Evaluating with test data\n",
    "    test_loss, test_acc = model.evaluate(x_test,y_test)\n",
    "\n",
    "    # Get weight and bias\n",
    "    w = model.get_weights()[0]\n",
    "    b = model.get_weights()[1]\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(weight, bias) = getWeight()\n",
    "getInfo(weight)\n",
    "getInfo(bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8ec53252cf26a957edbb8b9ce183c13794d5a62a045e83224f93ccf6391a87e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
