{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? centered in the downtown and out ? of detroit this comedy i found to be a terrific new comedic duo ? pat ? is a very funny man who happens to be a cop from japan on the trail of an industrial secrets thief who has stolen a ? ? ? super ? reluctantly he goes to the united states to follow the thief after being ordered by his commander ? character ? with ? ? character a fast ? but down to business player type detroit cop when they cross paths though the honorable ? of japan meet the all out old school detroit police ? ? the two stumble and trip over each other at first but then develop a ? that turns into an explosive two layered ? team that ? the case cold after battling a city crime boss for the stolen ? and closing the case these two go from ? each other to being friends and working well together a little worse for wear and in need of an extended vacation on top of it all they manage to come to a ? closing i rated this a 9 ? direction ? this a near perfect comedy fun for all ages i recommend it highly\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "# data loading (most frequent 10k words only)\n",
    "(train_data, train_lables), (test_data, test_lables) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# word & index dictionary mapping\n",
    "word_index = imdb.get_word_index() # type = dict\n",
    "\n",
    "# reversed dictionary mapping\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Decoding the review\n",
    "# 0 -> padding, 1 -> starts of sequence, 2 -> unknown\n",
    "decoded_review = \" \".join([reverse_word_index.get(i-3, \"?\") for i in train_data[819]])\n",
    "\n",
    "print(decoded_review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ? centered in the downtown and out ? of detroit this comedy i found to be a terrific new comedic duo ? pat ? is a very funny man who happens to be a cop from japan on the trail of an industrial secrets thief who has stolen a ? ? ? super ? reluctantly he goes to the united states to follow the thief after being ordered by his commander ? character ? with ? ? character a fast ? but down to business player type detroit cop when they cross paths though the honorable ? of japan meet the all out old school detroit police ? ? the two stumble and trip over each other at first but then develop a ? that turns into an explosive two layered ? team that ? the case cold after battling a city crime boss for the stolen ? and closing the case these two go from ? each other to being friends and working well together a little worse for wear and in need of an extended vacation on top of it all they manage to come to a ? closing i rated this a 9 ? direction ? this a near perfect comedy fun for all ages i recommend it highly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN의 input dimension은 항상 고정되어 있어야 합니다. 그러나 지금 input 으로 들어오는 text data의 경우 len(text data)의 값이 모두 다르기 때문에, text data를 크기가 고정된 무언가로 변환해야하는데, 아래에 정의된 vectorize_sequences() 가 이 역할을 수행해줍니다.\n",
    "먼저 shape이 (len(sequences), dimension) 인 zero matrix 를 만든 후, 각각의 txt data에서 해당 단어가 존재하는 위치를 1이 되도록 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing function (One-Hot Encoding)\n",
    "# Transform a review to a vector data\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # initializing the storing space\n",
    "    # Zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequences in enumerate(sequences):\n",
    "        # set specific indices of results[i] to 1\n",
    "        results[i, sequences] = 1\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss & plot_acc definition\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_acc(h, title='accuracy'):\n",
    "    plt.plot(h.history['accuracy'])\n",
    "    plt.plot(h.history['val_accuracy'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training', 'Validation'], loc=0)\n",
    "\n",
    "def plot_loss(h, title='loss'):\n",
    "    plt.plot(h.history['loss'])\n",
    "    plt.plot(h.history['val_loss'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training', 'Validation'], loc=0)\n",
    "# Vectorize\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "# re-format the data type (float data type as an inputs of MLP)\n",
    "# y = positive(1) of negative(0)\n",
    "y_train =np.asarray(train_lables).astype(\"float32\")\n",
    "y_test =np.asarray(test_lables).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4012, 11, 4, 9403]\n",
      "['?', 'centered', 'in', 'the', 'downtown']\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "word_index_of_X = train_data[819][0:5]\n",
    "print(word_index_of_X)\n",
    "print([reverse_word_index.get(i-3, \"?\") for i in word_index_of_X])\n",
    "print(x_train[819][word_index_of_X[1]])\n",
    "print(x_train[819][word_index_of_X[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hsyoon/miniforge3/envs/tf/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 06:24:52.983028: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.7845"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 06:24:54.441059: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 48ms/step - loss: 0.5052 - accuracy: 0.7845 - val_loss: 0.3748 - val_accuracy: 0.8713\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.2987 - accuracy: 0.9030 - val_loss: 0.3021 - val_accuracy: 0.8890\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2231 - accuracy: 0.9289 - val_loss: 0.2776 - val_accuracy: 0.8910\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.1777 - accuracy: 0.9425 - val_loss: 0.2801 - val_accuracy: 0.8877\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.1452 - accuracy: 0.9538 - val_loss: 0.2799 - val_accuracy: 0.8882\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.1200 - accuracy: 0.9629 - val_loss: 0.2909 - val_accuracy: 0.8854\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.1010 - accuracy: 0.9690 - val_loss: 0.3154 - val_accuracy: 0.8826\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.0829 - accuracy: 0.9771 - val_loss: 0.3265 - val_accuracy: 0.8799\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.0695 - accuracy: 0.9806 - val_loss: 0.3481 - val_accuracy: 0.8799\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.0565 - accuracy: 0.9856 - val_loss: 0.3705 - val_accuracy: 0.8808\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.0467 - accuracy: 0.9882 - val_loss: 0.4018 - val_accuracy: 0.8779\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.0389 - accuracy: 0.9913 - val_loss: 0.4278 - val_accuracy: 0.8737\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.0304 - accuracy: 0.9935 - val_loss: 0.4533 - val_accuracy: 0.8746\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.0253 - accuracy: 0.9949 - val_loss: 0.4817 - val_accuracy: 0.8733\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.0179 - accuracy: 0.9975 - val_loss: 0.5104 - val_accuracy: 0.8715\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.5570 - val_accuracy: 0.8683\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0120 - accuracy: 0.9986 - val_loss: 0.5831 - val_accuracy: 0.8684\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.0097 - accuracy: 0.9982 - val_loss: 0.6406 - val_accuracy: 0.8622\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.0052 - accuracy: 0.9999 - val_loss: 0.6527 - val_accuracy: 0.8673\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.0066 - accuracy: 0.9993 - val_loss: 0.6851 - val_accuracy: 0.8675\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.7518 - accuracy: 0.8513\n",
      "[0.751843273639679, 0.8512799739837646]\n",
      " 56/782 [=>............................] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 06:25:10.356840: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Modeling\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid')) # output is 0 or 1\n",
    "\n",
    "\n",
    "# model.complie(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# customizing learning rate\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# test validation split (10000 examples)\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]\n",
    "# model learning\n",
    "history = model.fit(partial_x_train, partial_y_train, \n",
    "                    epochs=20, batch_size=512, validation_data=(x_val, y_val))\n",
    "\n",
    "# model evaluation\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 마치고 결과를 확인하기 위해서는 keras package 안에 있는 model.predict() 를 사용하면 됩니다.\n",
    "25000개의 test dataset(x_test) 에 대응하는 결과로 긍정(1) 또는 부정(0)을 나타내는 25000개의 output dataset으로 표현되는 것을 확인했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step\n",
      "25000\n",
      "[[0.00504884]\n",
      " [0.99999976]\n",
      " [0.99662364]\n",
      " ...\n",
      " [0.00290497]\n",
      " [0.01071857]\n",
      " [0.690046  ]]\n"
     ]
    }
   ],
   "source": [
    "# model prediction\n",
    "predict_results = model.predict(x_test)\n",
    "print(len(predict_results))\n",
    "print(predict_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "819번째 리뷰를 제가 직접 읽어봤을 때 \"fun\", \"recommend\", \"perfect comedy\" 라는 단어가 있는 것으로 보아 긍정적인 리뷰라고 판단했습니다.\n",
    "모델이 예측한 결과를 확인하기 위해 model.predixt(x_test) 를 predict_results 라는 새로운 변수에 할당시키고, 819번째 결과값을 출력했습니다.\n",
    "결과는 0.9999989 로, 모델이 predict 한 값도 마찬가지로 긍정적인 리뷰라고 판단했으며 제가 생각한 결과와 일치하는 것을 확인했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999989]\n"
     ]
    }
   ],
   "source": [
    "# X th text sample output\n",
    "print(predict_results[819])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plot_loss(history)\n",
    "plt.show()\n",
    "# plt.savefig('chapter2-1.loss.png')\n",
    "\n",
    "plt.clf()\n",
    "plot_acc(history)\n",
    "plt.show()\n",
    "# plt.savefig('chapter2-1.accuracy.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, \n",
    "                    epochs=20, batch_size=512, validation_data=(x_val, y_val), \n",
    "                    callbacks = [EarlyStopping(monitor='val_loss', patience=1)])\n",
    "\n",
    "# visualization\n",
    "plot_loss(history)\n",
    "plt.show()\n",
    "# plt.savefig('chapter2-1.loss.png')\n",
    "\n",
    "plt.clf()\n",
    "plot_acc(history)\n",
    "plt.show()\n",
    "# plt.savefig('chapter2-1.accuracy.png')\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8ec53252cf26a957edbb8b9ce183c13794d5a62a045e83224f93ccf6391a87e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
